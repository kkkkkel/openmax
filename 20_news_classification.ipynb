{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6raXGM0Pawy"
      },
      "source": [
        "# Classifying news from 20NewsGroup\n",
        "**Author**: [albertobas](https://www.github.com/albertobas)<br />**Date created**: 10/03/2021<br />**Description**: text classification on the 20NewsGroup dataset.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/albertobas/20-news-classification/blob/main/20_news_classification.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "<br />\n",
        "___\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook I train convolutional and recurrent neural networks to classify sequences. The sequences used are the result of vectorizing messages obtained from the [20NewsGroup](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups) dataset, which contains 20000 messages taken from 20 newsgroups.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aoABxTHzPaw6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from utils import get_data, get_embedding_index, train, TwentyNewsDataset, Vectorizer\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "SEED = 29\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkAWsLBePaw7"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "scrolled": true,
        "id": "phNpbIu9Paw8",
        "outputId": "3ca98363-4b12-4ad3-dfa7-72fdf21e3b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20_newsgroups.tar.gz already exists\n",
            "Extracting 20_newsgroups.tar.gz\n",
            "Reading files\n"
          ]
        }
      ],
      "source": [
        "texts, labels = get_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg-MMsMmPaw9"
      },
      "source": [
        "The messages have several headers at the top. One of them is the category field which states the label of the observation. The label may also be present in other fields in the headers, namely _Followup-to_ or _Subject_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GjiF2WlUPaw-",
        "outputId": "1f917108-d55a-491c-a408-457f58e880fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> things which are eternal.  Jesus is a subset of God.   Therefore\n",
            ">:> Jesus belongs to the set of things which are eter\n"
          ]
        }
      ],
      "source": [
        "sample = open(os.path.join(os.getcwd(), 'data', '20_newsgroups',\n",
        "              \"alt.atheism\", \"51198\"), encoding=\"latin-1\").read()\n",
        "print(sample[890:1010]) # edit start and end indexes to see more text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2q5sg4TPaw_"
      },
      "source": [
        "When [reading the files](utils/preprocessing.py), any header with a mention of the corresponding label is thus removed. Furthermore, the _Path_ and _Xref_ headers and the headers that contain e-mail addresses are also removed.\n",
        "\n",
        "The texts are split to form a training data set and a validation set.\n",
        "\n",
        "I use `train_test_split` to stratify the split and have a proportionate distribution of categories among both sets, and leave the last observation out of the training dataset to make a prediction for the sake of representation at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oEDSlEmXPaw_"
      },
      "outputs": [],
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(texts[:-1], labels[:-1],\n",
        "                                                  random_state=SEED,\n",
        "                                                  test_size=0.2,\n",
        "                                                  stratify=labels[:-1])\n",
        "x_sample, y_sample = texts[-1], labels[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "d0FeOANhPaxA"
      },
      "outputs": [],
      "source": [
        "category_index = {c: i for i, c in enumerate(sorted(set(y_train)))}\n",
        "index_category = {category_index[k]: k for k in category_index.keys()}\n",
        "y_train = [category_index[c] for c in y_train]\n",
        "y_val = [category_index[c] for c in y_val]\n",
        "\n",
        "OUTPUT_DIM = len(category_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "852Shi2iPaxB"
      },
      "source": [
        "I have written a [vectorizer](utils/preprocessing.py) to transform the texts to sequences. The method is simple, first an object is instantiated, set in this case with a max length per sequence of 200 tokens and a maximum of 20000 tokens in the vocabulary. \n",
        "\n",
        "Then the `fit` method of the object takes the training array as an argument to form the vocabulary which  will be used every time an array is passed to the `transform` method. Transforming an array is tokenizing a string and vectorizing it mapping from the vocabulary.\n",
        "\n",
        "The resulting arrays of sequences alongside with their corresponding target lists are then passed to `TwentyNewsDataset` to instantiate two `torch.utils.data.Dataset` objects, which will be used in the training of the neural nets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6TyREujGPaxC"
      },
      "outputs": [],
      "source": [
        "# Text parameters\n",
        "MAX_LEN = 200 \n",
        "VOCAB_SIZE = 20000\n",
        "\n",
        "v = Vectorizer(max_len=MAX_LEN, n_words=VOCAB_SIZE, normalize=False)\n",
        "x_train_vectorized = v.fit_transform(x_train)\n",
        "x_val_vectorized = v.transform(x_val)\n",
        "\n",
        "if v.max_len is None:\n",
        "    MAX_LEN = x_train_vectorized.size(1)\n",
        "# The number of tokens is the total number of words in the vocabulary plus two \n",
        "# additional characters: <pad> and <unk>  \n",
        "N_TOKENS = v.vocab_size\n",
        "\n",
        "frame_train = TwentyNewsDataset(x_train_vectorized, torch.LongTensor(y_train))\n",
        "frame_val = TwentyNewsDataset(x_val_vectorized, torch.LongTensor(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4XaUog9PaxD"
      },
      "source": [
        "> A lower overfitting to the training set, and thus higher validation accuracy, may be found increasing the maximum sequence length. For performance reasons I have set a relatively low length.\n",
        "\n",
        "## Modelling\n",
        "In this section four models are trained, two convolutional networks and two recurrent networks, using [this functions](utils/training.py).\n",
        "\n",
        "All of the models implemented share a similar workflow, i.e., a multidimensional representation of the sequences, a process of learning high-level features in the data and a last phase of classifying the estimations.\n",
        "\n",
        "For this reason I have written a simple object that I will instantiate in every model and which will gather the different modules to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "o6kRI6fxPaxD"
      },
      "outputs": [],
      "source": [
        "class TwentyNewsNet(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding, module, classifier, is_recurrent=False):\n",
        "        super(TwentyNewsNet, self).__init__()\n",
        "        self.embedding = embedding\n",
        "        self.module = module\n",
        "        self.classifier = classifier\n",
        "        self.is_recurrent = is_recurrent\n",
        "        self.pretrained_weights = pretrained_weights\n",
        "        modules = [x for x in self.modules() if isinstance(x, nn.Conv1d) | isinstance(x, nn.Conv2d) |\n",
        "                   isinstance(x, nn.LSTM) | isinstance(x, nn.GRU) | isinstance(x, nn.Embedding) |\n",
        "                   isinstance(x, nn.Linear)]\n",
        "        self._init_wnb(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.is_recurrent:\n",
        "            seq_lens, idx_sort = torch.LongTensor([seq for seq in\n",
        "                                                   torch.sum((x > 0), dim=1)]).sort(0, descending=True)\n",
        "            idx_unsort = np.argsort(idx_sort)\n",
        "            x = x[idx_sort]\n",
        "            x = self.embedding(x)\n",
        "            x = self.module(x, seq_lens, idx_unsort)\n",
        "        else:\n",
        "            x = self.embedding(x)\n",
        "            x = self.module(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "    def _init_wnb(self, *args):\n",
        "        init_range = 0.05\n",
        "        init_constant = 0\n",
        "        for module in args:\n",
        "            for name, param in module.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    if isinstance(module, nn.Embedding):\n",
        "                        # from_pretained embedding is frozen\n",
        "                        if module.weight.requires_grad:\n",
        "                            nn.init.uniform_(\n",
        "                                param.data, -init_range, init_range)\n",
        "                    else:\n",
        "                        nn.init.xavier_uniform_(param.data)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.constant_(param.data, init_constant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-v20BkVPaxE"
      },
      "source": [
        "### Embedding\n",
        "\n",
        "The following class will be used to apply a multidimensional transformation to the sequences in the forward pass. The backward pass will only alter the weights of the embeddings if these are not pre trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "7cidTK4DPaxF"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_embeddings, embedding_dim, dropout=0, pretrained_weights=None, is_permute=False):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.is_permute = is_permute\n",
        "        if pretrained_weights is None:\n",
        "            self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=v.word_index[v.pad_token])\n",
        "        else:\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_weights, freeze=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # (N, L) -> (N, L, H)\n",
        "        x = self.embedding((x))\n",
        "        x = self.dropout(x)\n",
        "        if self.is_permute:\n",
        "            # (N, L, H) -> (N, H, L)\n",
        "            return x.permute(0, 2, 1)\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vy9vFHsPaxF"
      },
      "source": [
        "As I use pretrained weights in one of the convolutional neural networks later, I download a GloVe index (822 MB) with all the weights and create a tensor with the shape (20002, 100) or (`N_TOKENS`, `GLOVE_EMBEDDING_DIM`). In other words, a matrix with all the words in the vocabulary plus two special characters, and the corresponding 100 dimensions for each token.\n",
        "\n",
        "If the GloVe index does not contain a token from the vocabulary, every dimension of that token will equal zero."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ZipFile"
      ],
      "metadata": {
        "id": "Be6nBoukW-TP",
        "outputId": "2811c68e-1787-4489-e92a-41c1455c5b02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ZipFile (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for ZipFile\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "kYMwmxifPaxG",
        "outputId": "19c33c4d-18a4-4608-cf57-3c3d5dd3a28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-e3fd69692a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'd.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "GLOVE_EMBEDDING_DIM = 100\n",
        "found, missed = 0, 0\n",
        "missed_list = []\n",
        "embedding_dim = 100\n",
        "\n",
        "# embeddings_index = get_embedding_index(GLOVE_EMBEDDING_DIM)\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join('glove.6B.' + str(embedding_dim) + 'd.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "pretrained_weights = np.zeros(\n",
        "    (N_TOKENS, GLOVE_EMBEDDING_DIM)).astype(\"float32\")\n",
        "# pretrained_weights = np.random.uniform(\n",
        "#     -1, 1, (N_TOKENS, GLOVE_EMBEDDING_DIM)).astype(\"float32\")\n",
        "\n",
        "for idx, word in v.index_word.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        pretrained_weights[idx] = embedding_vector\n",
        "        found += 1\n",
        "    else:\n",
        "        missed += 1\n",
        "        missed_list.append(word)\n",
        "\n",
        "pretrained_weights = torch.from_numpy(pretrained_weights)\n",
        "print(\"Found %d words. Missed %d.\" % (found, missed))\n",
        "print(missed_list[:8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg209rwAPaxG"
      },
      "source": [
        "More than 90% of the tokens have been assigned pretrained weights, some of the tokens that have not been found in the GloVe index are shown above.\n",
        "\n",
        "### Classifier\n",
        "I have defined two classifiers, one with one layer fully connected and another one with two. Both include a dropout argument to randomly zero out a percentage of the elements of the input tensor to the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuZelyjEPaxH"
      },
      "outputs": [],
      "source": [
        "class OneLayerClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, dropout=0, log_softmax=True):\n",
        "        super(OneLayerClassifier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        if self.log_softmax:\n",
        "            # (N, Hin) -> (N, Hout)\n",
        "            return F.log_softmax(self.fc1(x), dim=1)\n",
        "        else:\n",
        "            # (N, Hin) -> (N, Hout)\n",
        "            return self.fc1(x)\n",
        "\n",
        "\n",
        "class TwoLayerClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0, log_softmax=True):\n",
        "        super(TwoLayerClassifier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        print('fc1',self.fc1.shape)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        print(self.fc2.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (N, Hin) -> (N, Hout)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        if self.log_softmax:\n",
        "            # (N, Hin) -> (N, Hout)\n",
        "            return F.log_softmax(self.fc2(x), dim=1)\n",
        "        else:\n",
        "            # (N, Hin) -> (N, Hout)\n",
        "            return self.fc2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5DGvURcPaxH"
      },
      "outputs": [],
      "source": [
        "# torch.utils.data.DataLoader parameters\n",
        "params_data_loader = {'batch_size': 128, 'shuffle': True, 'num_workers': 2, 'drop_last': False}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIdzIPh5PaxI"
      },
      "source": [
        "### CNN\n",
        "Two simple models with convolutional networks are trained:\n",
        "- the first version is a network with three one-dimension convolution layers with max pooling, which uses as input embeddings with pretrained weights of 100 dimensions, and which has a two-layer classifier in the output. \n",
        "\n",
        "- the second version is a network with four 1-d convolution layers with max pooling, an input embedding layer of 128-d, and an output of a one-layer classifier.\n",
        "\n",
        "I have set L2 regularization, and 50% and 40% dropout in every respective classifier to try to address an issue of overfitting the training set. This has a better result in the second convolutional network.\n",
        "\n",
        "The hyperparameters are listed at the top of each respective notebook cell.\n",
        "#### CNN v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lc3XxaMPaxI"
      },
      "outputs": [],
      "source": [
        "class CNNv1(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, channels, kernel_sizes, window_sizes):\n",
        "        super(CNNv1, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.conv1 = nn.Conv1d(input_dim, channels[0], kernel_sizes[0])\n",
        "        print(self.conv1.shape)\n",
        "        self.pool1 = nn.MaxPool1d(window_sizes[0])\n",
        "        print(self.pool1.shape)\n",
        "        self.conv2 = nn.Conv1d(channels[0], channels[1], kernel_sizes[1])\n",
        "        print(self.conv2.shape)\n",
        "        self.pool2 = nn.MaxPool1d(window_sizes[1])\n",
        "        print(self.pool2.shape)\n",
        "        self.conv3 = nn.Conv1d(channels[1], channels[2], kernel_sizes[2])\n",
        "        print(self.conv3.shape)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # (N, H, Lin) -> (N, Cout, Lout)\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        # (N, Cin, Lin) -> (N, Cout, Lout)\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        # (N, Cin, Lin) -> (N, Cout)\n",
        "        return torch.max(F.relu(self.conv3(x)), 2).values\n",
        "    \n",
        "    def input_dim(self):\n",
        "        return self.channels[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "465M8vs1PaxI"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "CHANNELS = [128, 128, 128]\n",
        "HIDDEN_DIM = 128\n",
        "KERNEL_SIZES = [5, 5, 5]\n",
        "WINDOW_SIZES = [5, 5]\n",
        "DROPOUT = 0.5\n",
        "\n",
        "\n",
        "cnn_v1_embedding = Embedding(N_TOKENS, GLOVE_EMBEDDING_DIM, pretrained_weights=pretrained_weights,\n",
        "                             is_permute=True)\n",
        "cnn_v1_module = CNNv1(GLOVE_EMBEDDING_DIM, CHANNELS,\n",
        "                      KERNEL_SIZES, WINDOW_SIZES)\n",
        "cnn_v1_classifier = TwoLayerClassifier(\n",
        "    cnn_v1_module.input_dim(), HIDDEN_DIM, OUTPUT_DIM, DROPOUT)\n",
        "cnn_v1 = TwentyNewsNet(cnn_v1_embedding, cnn_v1_module, cnn_v1_classifier)\n",
        "\n",
        "# Training parameters\n",
        "params_train = {'data_loader': {'batch_size': 128,\n",
        "                                'shuffle': True, 'num_workers': 2, 'drop_last': False}}\n",
        "params_train['epochs'] = 12\n",
        "params_train['criterion'] = {'name': 'nll_loss'}\n",
        "params_train['optimizer'] = {'name': 'Adam',\n",
        "                             'config': {'lr': 1e-3, 'weight_decay': 1e-3}}\n",
        "params_train['scheduler'] = {'name': 'ReduceLROnPlateau',\n",
        "                             'config': {'factor': 0.75, 'min_lr': 5e-4, 'mode': 'max', 'patience': 0},\n",
        "                             'step': {'metric': 'val_acc'}}\n",
        "params_eval = {k: params_train['data_loader'][k]\n",
        "               for k in params_train['data_loader'].keys() if k != 'shuffle'}\n",
        "\n",
        "train(cnn_v1, frame_train, frame_val, params_train, params_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqtyJPlqPaxJ"
      },
      "source": [
        "#### CNN v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxiAbCHxPaxJ"
      },
      "outputs": [],
      "source": [
        "class CNNv2(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, channel, kernel_sizes, max_len, is_batch_norm=False):\n",
        "        super(CNNv2, self).__init__()\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv1d(1, channel, (K, input_dim)) for K in kernel_sizes])\n",
        "        self.pools = nn.ModuleList(\n",
        "            [nn.MaxPool1d(self._conv_out_shape(conv, max_len)) for conv in self.convs])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [(N, L, H) -> (N, 1, L, H) -> (N, Cout, L, 1) -> (N, Cout, L) -> (N, Cout, 1) -> (N, Cout), ...]\n",
        "        x = [self.pools[i](F.relu(self.convs[i](x.unsqueeze(1)).squeeze(3))).squeeze(2)\n",
        "             for i in range(len(self.convs))]\n",
        "        # (N, Cout) -> (N, Cout x len(self.convs))\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "    def _conv_out_shape(self, conv, max_len):\n",
        "        return ((max_len + (2 * conv.padding[0]) - (conv.dilation[0] * (conv.kernel_size[0] - 1)) - 1) //\n",
        "                conv.stride[0]) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG6T2Z1yPaxJ"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "CHANNELS = 128\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "KERNEL_SIZES = [1, 2, 3, 4]\n",
        "DROPOUT = 0.4\n",
        "\n",
        "cnn_v2_embedding = Embedding(N_TOKENS, EMBEDDING_DIM, is_permute=False)\n",
        "cnn_v2_module = CNNv2(EMBEDDING_DIM, CHANNELS, KERNEL_SIZES, MAX_LEN)\n",
        "cnn_v2_classifier = OneLayerClassifier(\n",
        "    len(KERNEL_SIZES)*CHANNELS, OUTPUT_DIM, DROPOUT)\n",
        "cnn_v2 = TwentyNewsNet(cnn_v2_embedding, cnn_v2_module, cnn_v2_classifier)\n",
        "\n",
        "# Training parameters\n",
        "params_train.clear()\n",
        "params_train = {'data_loader': {'batch_size': 128,\n",
        "                                'shuffle': True, 'num_workers': 2, 'drop_last': False}}\n",
        "params_train['epochs'] = 12\n",
        "params_train['criterion'] = {'name': 'nll_loss'}\n",
        "params_train['optimizer'] = {'name': 'Adam',\n",
        "                             'config': {'lr': 1e-3, 'weight_decay': 1e-3}}\n",
        "params_train['scheduler'] = {'name': 'ReduceLROnPlateau',\n",
        "                             'config': {'factor': 0.75, 'min_lr': 5e-4, 'mode': 'max', 'patience': 0},\n",
        "                             'step': {'metric': 'val_acc'}}\n",
        "params_eval.clear()\n",
        "params_eval = {k: params_train['data_loader'][k]\n",
        "               for k in params_train['data_loader'].keys() if k != 'shuffle'}\n",
        "\n",
        "train(cnn_v2, frame_train, frame_val, params_train, params_eval)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFBetWggPaxJ"
      },
      "source": [
        "### RNN\n",
        "Lastly, I have trained two recurrent neural networks:\n",
        "- One with a bidirectional one-layer LSTM and a self attention mechanism.\n",
        "\n",
        "- And another one with a bidirectional one-layer GRU and a self attention mechanism.\n",
        "\n",
        "Both have an embedding layer of 128 dimensions and a one-layer classifier. However I have only set dropout in the LSTM.\n",
        "\n",
        "We achieve a better validation accuracy with the recurrent networks, although there appears to be a more remarkable overfitting as we can see on the graphs below during the learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y2OHxDZPaxK"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, rnn_type='LSTM', n_layers=1, is_bidirectional=True,\n",
        "                 is_batch_first=True):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.is_bidirectional = is_bidirectional\n",
        "        self.is_batch_first = is_batch_first\n",
        "        self.num_directions = 2 if is_bidirectional else 1\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn_type = rnn_type\n",
        "            self.rnn = getattr(nn, rnn_type)(input_dim, hidden_dim, n_layers, bidirectional=is_bidirectional,\n",
        "                                             batch_first=self.is_batch_first)\n",
        "        else:\n",
        "            raise ValueError(\"The rnn_type allowed are 'LSTM' and 'GRU'.\")\n",
        "        self.attention = Attention(self.output_dim())\n",
        "\n",
        "    def forward(self, x, seq_lens, idx_unsort):\n",
        "        batch_size, total_length = x.size(0), x.size(1)\n",
        "        x = pack_padded_sequence(x, seq_lens, batch_first=self.is_batch_first)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = pad_packed_sequence(\n",
        "            x, batch_first=self.is_batch_first, total_length=total_length)[0]\n",
        "        return self.attention(x[idx_unsort])\n",
        "\n",
        "    def output_dim(self):\n",
        "        return self.hidden_dim * self.num_directions\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (N, L, Hin) -> (N, L, Hout)\n",
        "        energy = torch.tanh(self.fc1(x))\n",
        "        # (N, L, Hin) -> (N, L, 1)\n",
        "        energy = self.fc2(x)\n",
        "        # (N, L, 1) -> (N, L)\n",
        "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
        "        # (N, L, Hin) x (N, L, 1) -> (N, Hin)\n",
        "        return(x * weights.unsqueeze(-1)).sum(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNUBvBbPaxK"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHgxxfPpPaxK"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 100\n",
        "DROPOUT = 0.4\n",
        "\n",
        "lstm_v2_embedding = Embedding(N_TOKENS, EMBEDDING_DIM)\n",
        "lstm_v2_module = RNN(EMBEDDING_DIM, HIDDEN_DIM,\n",
        "                     rnn_type='LSTM', is_bidirectional=True)\n",
        "lstm_v2_classifier = OneLayerClassifier(\n",
        "    lstm_v2_module.output_dim(), OUTPUT_DIM, DROPOUT)\n",
        "lstm = TwentyNewsNet(lstm_v2_embedding, lstm_v2_module,\n",
        "                     lstm_v2_classifier, is_recurrent=True)\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "params_train.clear()\n",
        "params_train = {'data_loader': params_data_loader}\n",
        "params_train['epochs'] = 8\n",
        "params_train['criterion'] = {'name': 'nll_loss'}\n",
        "params_train['optimizer'] = {'name': 'Adam',\n",
        "                             'config': {'lr': 1.25e-3}}\n",
        "params_train['scheduler'] = {'name': 'ReduceLROnPlateau',\n",
        "                             'config': {'factor': 0.75, 'min_lr': 5e-4, 'mode': 'max', 'patience': 0},\n",
        "                             'step': {'metric': 'val_acc'}}\n",
        "params_eval.clear()\n",
        "params_eval = {k: params_train['data_loader'][k]\n",
        "               for k in params_train['data_loader'].keys() if k != 'shuffle'}\n",
        "\n",
        "train(lstm, frame_train, frame_val, params_train, params_eval)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8vVKwY6PaxL"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O55qX-pDPaxL"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 100\n",
        "\n",
        "gru_embedding = Embedding(N_TOKENS, EMBEDDING_DIM, is_permute=False)\n",
        "gru_module = RNN(EMBEDDING_DIM, HIDDEN_DIM,\n",
        "                 rnn_type='GRU', is_bidirectional=True)\n",
        "gru_classifier = OneLayerClassifier(\n",
        "    gru_module.output_dim(), OUTPUT_DIM, DROPOUT)\n",
        "gru = TwentyNewsNet(gru_embedding, gru_module,\n",
        "                    gru_classifier, is_recurrent=True)\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "params_train.clear()\n",
        "params_train = {'data_loader': params_data_loader}\n",
        "params_train['epochs'] = 6\n",
        "params_train['criterion'] = {'name': 'nll_loss'}\n",
        "params_train['optimizer'] = {'name': 'Adam',\n",
        "                             'config': {'lr': 1e-3}}\n",
        "params_train['scheduler'] = {'name': 'ReduceLROnPlateau',\n",
        "                             'config': {'factor': 0.75, 'min_lr': 5e-4, 'mode': 'max', 'patience': 0},\n",
        "                             'step': {'metric': 'val_acc'}}\n",
        "params_eval.clear()\n",
        "params_eval = {k: params_train['data_loader'][k]\n",
        "               for k in params_train['data_loader'].keys() if k != 'shuffle'}\n",
        "\n",
        "train(gru, frame_train, frame_val, params_train, params_eval)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ubFGMJTPaxL"
      },
      "source": [
        "Increasing the regularization does not remedy this issue. Therefore, a solution could be to increase the length of the sequences.\n",
        "\n",
        "## Predicting\n",
        "We can see below the workflow used to make a single prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHz-PRl_PaxL"
      },
      "outputs": [],
      "source": [
        "def predict(model, data):\n",
        "    model.eval()\n",
        "    _, y_pred = torch.max(model(v.transform(data)), 1)\n",
        "    return index_category[y_pred.item()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ADSs2QiPaxL"
      },
      "outputs": [],
      "source": [
        "x_sample[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nz0FWg_VPaxM"
      },
      "outputs": [],
      "source": [
        "format_string = \"{}{}\"\n",
        "print(format_string.format('Model'.ljust(10), 'Prediction'))\n",
        "print(format_string.format('-----'.ljust(10), '----------'))\n",
        "\n",
        "for model in zip(['CNN_v1', 'CNN_v2', 'LSTM', 'GRU'], [cnn_v1, cnn_v2, lstm, gru]):\n",
        "    y_pred = predict(model[1], [x_sample])\n",
        "    print(format_string.format(model[0].ljust(10), y_pred))\n",
        "\n",
        "print(\"\\nLabel: {}\".format(y_sample))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}